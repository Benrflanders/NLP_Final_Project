{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_feature(word):\n",
    "\treturn {'word': word}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = opinion_lexicon\n",
    "\n",
    "positive_words = ([(lemmatizer.lemmatize(word=word), 1.0) for word in text.positive()])\n",
    "negative_words = ([(lemmatizer.lemmatize(word=word), 0.0) for word in text.negative()])\n",
    "\n",
    "labeled_words = positive_words + negative_words\n",
    "random.shuffle(labeled_words)\n",
    "\n",
    "featuresets = [(sentiment_feature(n[0]), n[1]) for n in labeled_words]\n",
    "\n",
    "train_amnt = int(len(featuresets)*.8) #80% train data; 20% test data\n",
    "train_data = featuresets[:train_amnt]\n",
    "test_data = featuresets[train_amnt:]\n",
    "assert len(train_data) == train_amnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  separate\n",
      "sentiment:  0.5\n",
      "probability of recieving a sentiment of 1.0:  0.5\n",
      "probability of recieving a sentiment of 0.0:  0.5\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist, ConditionalProbDist\n",
    "cfdist = ConditionalFreqDist(labeled_words)\n",
    "cpdist = ConditionalProbDist(cfdist, ELEProbDist, bins=2)\n",
    "\n",
    "word = 'separate'\n",
    "\n",
    "print(\"word: \", word)\n",
    "#print(\"Number of occurances in opinion_lexicon: \", cpdist[word].max())\n",
    "print(\"sentiment: \", (cpdist[word].prob(1.0) * 1))\n",
    "print(\"probability of recieving a sentiment of 1.0: \", (cpdist[word].prob(1.0) * 1))\n",
    "print(\"probability of recieving a sentiment of 0.0: \", (cpdist[word].prob(0.0) * 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import LaplaceProbDist, LidstoneProbDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.probability import LaplaceProbDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "\n",
    "from nltk.probability import ProbDistI, FreqDist\n",
    "\n",
    "\n",
    "freq_dist = {}\n",
    "\n",
    "#freqDist = FreqDist()\n",
    "#probDist = ProbDistI(freqDist)\n",
    "\n",
    "#dict value = [occurances of -1, occurances of 1]\n",
    "for wrd in labeled_words:\n",
    "    \n",
    "    default = 0\n",
    "    \n",
    "    wrd_tmp = str(wrd[0])\n",
    "    val = int(wrd[1])\n",
    "    \n",
    "    freq_dist[wrd_tmp] = [0,0]\n",
    "    \n",
    "    if(val == 1):\n",
    "        freq_dist[wrd_tmp][1] += 1\n",
    "    elif(val == -1):\n",
    "        freq_dist[wrd_tmp][0] += 1 \n",
    "    \n",
    "#LaplaceProbDist()\n",
    "\n",
    "\n",
    "\n",
    "#classifier = nltk.NaiveBayesClassifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, nltk\n",
    "from Models.LoadAndTrainModels import sentiment_feature_sentence, applyFeatureSetToBigrams, sentiment_feature\n",
    "\n",
    "data_path = os.path.join('/home/brf97486/finalProject/labelled_sentence_data_set/sentiment labelled sentences')\n",
    "yelp = os.path.join(data_path, 'yelp_labelled.txt')\n",
    "imdb = os.path.join(data_path, 'imdb_labelled.txt')\n",
    "amazon = os.path.join(data_path, 'amazon_cells_labelled.txt')\n",
    "assert os.path.isfile(yelp) \n",
    "assert os.path.isfile(imdb) \n",
    "assert os.path.isfile(amazon) \n",
    "file = open(yelp, 'r')\n",
    "file2 = open(imdb, 'r')\n",
    "file3 = open(amazon, 'r')\n",
    "dataset = []\n",
    "for line in file:\n",
    "    dataset.append(re.split(r'\\t+', line))\n",
    "for line in file2:\n",
    "    dataset.append(re.split(r'\\t+', line))\n",
    "for line in file3:\n",
    "    dataset.append(re.split(r'\\t+', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "#nltk.bigram(dataset)\n",
    "for line in dataset:\n",
    "    value = int(line[1])\n",
    "    words = nltk.word_tokenize(line[0])\n",
    "    bigram = list(nltk.bigrams(words))\n",
    "    #bigram_with_features = applyFeatureSetToBigrams(bigram)\n",
    "    for bi in bigram:\n",
    "       ds.append((value, ' '.join(bi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdist = ConditionalFreqDist(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "bigram_features = list(cfdist.keys())\n",
    "print(bigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You', 'can')\n",
      "('can', 'not')\n",
      "('not', 'answer')\n",
      "('answer', 'calls')\n",
      "('calls', 'with')\n",
      "('with', 'the')\n",
      "('the', 'unit')\n",
      "('unit', ',')\n",
      "(',', 'never')\n",
      "('never', 'worked')\n",
      "('worked', 'once')\n",
      "('once', '!')\n"
     ]
    }
   ],
   "source": [
    "featuresets = [print(bi) for bi in bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getExternalDatasets import *\n",
    "train, test, prob = getUCIDataset_bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfdist[('not good')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import DictionaryProbDist\n",
    "    \n",
    "dict_probs = {0.0: .1, 1.0 : .1}\n",
    "label_probdist = DictionaryProbDist(dict_probs)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier(label_probdist=label_probdist, feature_probdist=cfdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent:  47.17868338557994\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent: \", (nltk.classify.accuracy(classifier, test))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: -1.0, 1.0: -1.0}\n"
     ]
    }
   ],
   "source": [
    "prob_classify = classifier.prob_classify({'bigram': 'very good'})\n",
    "print(prob_classify._prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4f588e40f390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/apps/eb/nltk/3.4.4-foss-2018a-Python-3.6.4/lib/python3.6/site-packages/nltk-3.4.4-py3.6.egg/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mshow_most_informative_features\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Most Informative Features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mlabelprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apps/eb/nltk/3.4.4-foss-2018a-Python-3.6.4/lib/python3.6/site-packages/nltk-3.4.4-py3.6.egg/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mmost_informative_features\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mminprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Models import LoadAndTrainModels\n",
    "from Make_Enron_Corpus import enronCorpus\n",
    "from analyze_data.Data_Utils import *\n",
    "from getExternalDatasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent:  28.424153166421206\n",
      "average sentiment for each relation type:  {}\n",
      "classifying: envious\n",
      "{0.0: -1.0, 1.0: -1.0}\n",
      "classifying: good\n",
      "{0.0: -1.0, 1.0: -1.0}\n",
      "classifying: bad\n",
      "{0.0: -1.0, 1.0: -1.0}\n"
     ]
    }
   ],
   "source": [
    "word_set = LoadAndTrainModels.applyFeatureSetToWords(enronCorpus().words())\n",
    "\n",
    "\n",
    "#training_data, testing_data, prob_dist = LoadAndTrainModels.getOpinionTrainingDataWords()\n",
    "\n",
    "#get the trained sentiment classifier\n",
    "classifier = LoadAndTrainModels.getNaiveBayesTrainedClassifier(LoadAndTrainModels.getOpinionTrainingDataWordsAndProbDist)\n",
    "\n",
    "\n",
    "\n",
    "#classify each relationship type and store the average in a dict\n",
    "avg_sentiment = {}\n",
    "\n",
    "for d in enron_corp.fileids():\n",
    "    break\n",
    "    relation_data = enron_corp.words(d) #data for a single relation\n",
    "    relation_feature_data = LoadAndTrainModels.applyFeatureSetToWords(relation_data) #data for a single relation with a feature set application to it\n",
    "    print(type(relation_feature_data))\n",
    "    total_sentiment = 0\n",
    "    total_sentiment = classifier.classify_many(featuresets=relation_feature_data)\n",
    "    avg_sentiment[d] = sum(total_sentiment)/len(relation_feature_data)\n",
    "\n",
    "print(\"average sentiment for each relation type: \", avg_sentiment)\n",
    "\n",
    "print(\"classifying: envious\")\n",
    "prob_classify = (classifier.prob_classify({\"word\":\"envious\"}))\n",
    "print(prob_classify._prob_dict)\n",
    "\n",
    "print(\"classifying: good\")\n",
    "prob_classify = (classifier.prob_classify({\"word\":\"good\"}))\n",
    "print(prob_classify._prob_dict)\n",
    "\n",
    "print(\"classifying: bad\")\n",
    "prob_classify = (classifier.prob_classify({\"word\":\"bad\"}))\n",
    "print(prob_classify._prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
